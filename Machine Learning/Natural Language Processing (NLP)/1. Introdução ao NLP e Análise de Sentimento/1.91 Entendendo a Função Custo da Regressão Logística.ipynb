{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.91 Entendendo a Fun√ß√£o Custo da Regress√£o Log√≠stica\n",
    "Vamos entender melhor o que √© essa **fun√ß√£o custo** quando trabalhamos com Regress√£o Log√≠stica. O que acontece quando predizemos a _label_ (etiqueta) verdadeira? E quando predizemos a falsa?\n",
    "\n",
    "| Equa√ß√£o da Fun√ß√£o Custo|\n",
    "|:-:|\n",
    "|<img src='images/fun√ß√£o custo.png' width=100%>\n",
    "\n",
    "Essa √© a **equa√ß√£o da fun√ß√£o custo**. Um pouco assustadora? Na verdade pode ser entendida de uma forma simples, vamos nos aprofundar em cada um seus componentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No <span style=\"color:green\">**ret√¢ngulo verde**</span>:\n",
    "- podemos encontrar a **soma da vari√°vel \"m**, ou seja, o n√∫mero total de amostras de treino em seu _training dataset_ (como visto no guia anterior). Iremos realizar ent√£o a soma do custo de cada exemplo de treino. \n",
    "- o primeiro elemento, $-\\frac{1}{m}$, indica que quando combinado com a **soma**, ser√° uma esp√©cie de **m√©dia**. O sinal de **\"menos\"** indica que o total ser√° sempre **positivo**.\n",
    "\n",
    "\n",
    "Dentro das chaves (_brackets_) encontram-se duas linhas, **dois termos** que s√£o mesclados atrav√©s de uma soma.\n",
    "\n",
    "### Na <span style=\"color:red\">**linha vermelha**</span>:\n",
    "- temos o produto de **y** sobrescrito **i**, que √© a **etiqueta** (_label_) de cada exemplo de treino;\n",
    "- essa etiqueta √© ent√£o **multiplicada** pelo **log** da predi√ß√£o;\n",
    "- essa **predi√ß√£o** √© a fun√ß√£o da **regress√£o log√≠stica** $h(x ^i , \\theta)$ aplicada a cada exemplo de treino.\n",
    "    - Se a etiqueta for \"0\", a fun√ß√£o **h** poder√° retornar qualquer valor, portanto todo o termo ser√° **zero**.\n",
    "    - Se a etiqueta for \"1\" e a predi√ß√£o for **pr√≥xima de 1**, o log ser√° pr√≥ximo a zero (**log de 1 √© zero!**);\n",
    "    - Se a etiqueta for \"1\" e a predi√ß√£o for **pr√≥xima de 0**, o termo explode e se aproxima do infinito negativo $-\\infty$.\n",
    "    \n",
    "<img src='images/tabela cost function1.png' width=70%>\n",
    "        \n",
    "√â ent√£o que percebemos que:\n",
    "1. quando o **valor da predi√ß√£o for pr√≥ximo ao da etiqueta** (_label_), a perda (_loss_) √© **pequena**;\n",
    "2. quando a **predi√ß√£o e a etiqueta divergem**, o custo total (_overall cost_) **sobe**.\n",
    "\n",
    "\n",
    "### Na  <span style=\"color:blue\">**linha azul**</span>:\n",
    "Nesse caso,\n",
    "- se a etiqueta $y^i$ for \"1\", o termo **1 - y** ser√° \"0\". Portanto multiplicando pelo log, **o termo inteiro resultar√° \"0\"**;\n",
    "- se a etiqueta for \"0\" e a fun√ß√£o da regress√£o log√≠stica $h(x ^i , \\theta)$ for pr√≥xima a zero,  o produto ser√° **pr√≥ximo a zero**;\n",
    "- se a etiqueta for \"0\" e a predi√ß√£o pr√≥xima a \"1\", o **log** explode e se aproxima do infinito negativo $-\\infty$.\n",
    "\n",
    "<img src='images/tabela cost function2.png' width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.91.1 Intui√ß√µes\n",
    "Analisando atentamente os componentes dessa equa√ß√£o da fun√ß√£o custo, podemos concluir algumas coisas:\n",
    "1. Existe um termo na fun√ß√£o custo que ser√° relevante quando a etiqueta for **1** (linha vermelha);\n",
    "2. E outro que ser√° relevante quando a etiqueta for **0** (linha azul).\n",
    "3. Em cada um desses termos estamos pegando o **log** de um **valor entre zero e um**;\n",
    "4. Esses **logar√≠tmos** sempre retornam um **valor negativo**, e o sinal de \"menos\", indicado no quadrado verde, torna o custo total (_overall cost_) sempre **positivo**.\n",
    "\n",
    "\n",
    "### - Na predi√ß√£o, o que acontece com a perda (_loss_) quando a etiqueta √© 1?\n",
    "<img src='images/predi√ß√£o x custo.png' width=30%>\n",
    "\n",
    "No gr√°fico, podemos ver:\n",
    "1. no eixo horizontal o **valor de predi√ß√£o no eixo horizontal**;\n",
    "2. no eixo vertical o **custo** associado a um √∫nico exemplo de treinamento.\n",
    "\n",
    "\n",
    "**O J(ùúÉ) se reduz ao log negativo $h(x^{(i)}, \\theta)$**. \n",
    "1. Quando a predi√ß√£o √© pr√≥xima a \"1\", a perda √© pr√≥xima a \"0\". Isso acontece porque a predi√ß√£o converge bem com a etiqueta;\n",
    "2. Quando a predi√ß√£o √© pr√≥xima a \"0\", a perda se aproxima ao infinito (negativo). Isso acontece porque a predi√ß√£o diverge muito com a etiqueta.\n",
    "\n",
    "### - Na predi√ß√£o, o que acontece com a perda (_loss_) quando a etiqueta √© 0?\n",
    "\n",
    "<img src='images/predi√ß√£o x custo2.png' width=30%>\n",
    "\n",
    "Os eixos s√£o os mesmos do gr√°fico anterior, por√©m nesse caso, **o J(ùúÉ) se reduz ao log negativo $(1 - h(x^i, \\theta))$**. \n",
    "1. Quando a predi√ß√£o √© pr√≥xima a \"0\", a perda √© pr√≥xima a \"0\";\n",
    "2. Quando a predi√ß√£o √© pr√≥xima a \"1\", a perda se aproxima ao infinito (positivo).\n",
    "\n",
    "<img src='images/predi√ß√£o x custo3.png' width=70%>\n",
    "\n",
    "Portanto, esperamos que:\n",
    "1. quando a etiqueta for \"1\", a predi√ß√£o $h(x^{(i)}, \\theta)$ seja pr√≥xima de \"1\" e a perda pr√≥xima de \"0\";\n",
    "2. quando a etiqueta for \"0\", a predi√ß√£o $(1-h(x^{(i)}, \\theta))$ seja pr√≥xima de \"0\", e a perda pr√≥xima de \"0\".\n",
    "\n",
    "E se a etiqueta for igual √† predi√ß√£o? **O custo ser√° \"0\"**! Perfeito, agora j√° compreendemos melhor essa tal fun√ß√£o custo. Encerramos por aqui o assunto **Regress√£o Log√≠stica**!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
