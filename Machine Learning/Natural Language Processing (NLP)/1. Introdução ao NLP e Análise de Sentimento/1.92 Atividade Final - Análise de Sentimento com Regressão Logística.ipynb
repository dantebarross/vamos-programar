{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.92 Atividade Final - Análise de Sentimento com Regressão Logística\n",
    "Agora que já vimos a fundo como funciona a Regressão Logística e como podemos utilizá-la para predizer e classificar dados com base em um _dataset_ já anotado, podemos realizar a nossa atividade final. Vamos realizar a importação das nossas bibliotecas e funções, extrair recursos desse texto, aplicar a regressão logística, realizar o teste e então executar uma análise de erros!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos importar o NLTK\n",
    "import nltk\n",
    "from os import getcwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instale os pacotes do NLTK\n",
    "\n",
    "```Python\n",
    "nltk.download('twitter_samples')\n",
    "```\n",
    "```python\n",
    "nltk.download('stopwords')\n",
    "```\n",
    "\n",
    "#### Caso não tenha o arquivo utils.py (com as funções que criamos) em seu computador, ele se encontra nessa mesma pasta\n",
    "* `process_tweet()`: limpa o texto, transforma-o em palavras separadas (tokenização), remove palavras irrelevantes (stop words) e converte palavras em radicais (stem).\n",
    "* `build_freqs()`: isso conta quantas vezes uma palavra no 'corpus' (todo o conjunto de tweets) foi associada a um rótulo positivo '1' ou negativo '0', em seguida, constrói o dicionário `freqs`, onde cada chave é uma tupla (palavra, rótulo) e o valor é a contagem de sua frequência dentro do corpus de tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faça o download (está em nossa subpasta de NLP) e adicione o folder tmp2 como filePath. Vamos utilizar como corpora.\n",
    "\n",
    "filePath = f\"{getcwd()}/../tmp2/\"\n",
    "#filePath = \"/../tmp2/\" ou esse\n",
    "nltk.data.path.append(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import twitter_samples \n",
    "from utils import process_tweet, build_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparando dados\n",
    "* O twitter_samples contém subconjuntos de 5.000 tweets positivos, 5.000 tweets negativos e o conjunto completo de 10.000 tweets.\n",
    "* Vamos selecionar cinco mil tweets positivos e cinco mil tweets negativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecionando o _dataset_ positivo e negativo em variáveis\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**20% estarão no conjunto de teste e 80% no conjunto de treinamento.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividindo os dados em duas partes, uma para treinamento e outra para teste (conjunto de validação)\n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg \n",
    "test_x = test_pos + test_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Criando a matriz numpy de labels positivas e negativas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinando labels positivos e negativos\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y.shape = (8000, 1)\n",
      "test_y.shape = (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Print do \"shape\" do set de treino e teste\n",
    "print(\"train_y.shape = \" + str(train_y.shape))\n",
    "print(\"test_y.shape = \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Crie o dicionário de frequência usando a função importada `build_freqs ()`.\n",
    "     * É altamente recomendável que você abra `utils.py` e leia a função` build_freqs () `para entender o que está fazendo.\n",
    "\n",
    "```Python\n",
    "    for y,tweet in zip(ys, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "```\n",
    "* Observe como o loop \"for\" externo passa por cada tweet, e o loop \"for\" interno percorre cada palavra em um tweet.\n",
    "* O dicionário `freqs` é o dicionário de frequência que está sendo construído.\n",
    "* A chave é a tupla (word, label), como (\"happy\", 1) ou (\"happy\", 0). O valor armazenado para cada chave é a contagem de quantas vezes a palavra \"happy\" foi associada a um rótulo positivo ou quantas vezes \"happy\" foi associada a um rótulo negativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(freqs) = <class 'dict'>\n",
      "len(freqs) = 11340\n"
     ]
    }
   ],
   "source": [
    "# crie dicionário de frequências\n",
    "freqs = build_freqs(train_x, train_y)\n",
    "\n",
    "# output com type e len\n",
    "print(\"type(freqs) = \" + str(type(freqs)))\n",
    "print(\"len(freqs) = \" + str(len(freqs.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processando o tweet\n",
    "A função fornecida `process_tweet ()` tokeniza o tweet em palavras individuais, remove stop words e aplica lematização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of a positive tweet: \n",
      " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "\n",
      "This is an example of the processed version of the tweet: \n",
      " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "print('This is an example of a positive tweet: \\n', train_x[0])\n",
    "print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Regressão logística\n",
    "\n",
    "### Parte 1.1: Sigmóide\n",
    "* A função sigmóide é definida como:\n",
    "\n",
    "$$ h(z) = \\frac{1}{1+\\exp^{-z}} \\tag{1}$$\n",
    "\n",
    "Ele mapeia a entrada 'z' para um valor que varia entre 0 e 1 e, portanto, pode ser tratado como uma probabilidade.\n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='images/sigmoid_plot.jpg' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:300px;height:200px;\" /> Figura 1 </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos implementar a função sigmóide\n",
    "Procure saber sobre o numpy.exp\n",
    "<ul>\n",
    "    <li><a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html\" > numpy.exp </a> </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z): \n",
    "    '''\n",
    "    Input:\n",
    "         z: é a entrada (pode ser um escalar ou uma matriz)\n",
    "     Output:\n",
    "         h: o sigmóide de z\n",
    "    '''\n",
    "\n",
    "    # calculando sigmóide de z\n",
    "    h = 1 / (1 + np.exp(-z))  \n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS!\n",
      "CORRECT!\n"
     ]
    }
   ],
   "source": [
    "# Testando a função...\n",
    "if (sigmoid(0) == 0.5):\n",
    "    print('SUCCESS!')\n",
    "else:\n",
    "    print('Oops!')\n",
    "\n",
    "if (sigmoid(4.92) == 0.9927537604041685):\n",
    "    print('CORRECT!')\n",
    "else:\n",
    "    print('Oops again!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressão logística: regressão e um sigmóide\n",
    "\n",
    "A regressão logística faz uma regressão linear regular e aplica um sigmóide à saída da regressão linear.\n",
    "\n",
    "Regressão:\n",
    "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
    "Observe que os valores $\\theta$ são \"pesos\". Se você fez a especialização em Deep Learning, nos referimos aos pesos com o vetor `w`. Estamos usando uma variável diferente $\\theta$ para nos referir aos pesos.\n",
    "\n",
    "Regressão logística:\n",
    "$$ h(z) = \\frac{1}{1+\\exp^{-z}}$$\n",
    "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
    "'z' = 'logits'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 1.2 Função de custo e gradiente\n",
    "A função de custo usada para regressão logística é a média da perda de log em todos os exemplos de treinamento:\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)}))\\tag{5} $$\n",
    "* $m$ número de exemplos de treinamento\n",
    "* $y^{(i)}$ é o label real do i-ésimo exemplo de treinamento.\n",
    "* $h(z(\\theta)^{(i)})$ é a previsão do modelo para o i-ésimo exemplo de treinamento.\n",
    "\n",
    "A função de perda para um único exemplo de treinamento é:\n",
    "$$ Loss = -1 \\times \\left( y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) \\right)$$\n",
    "\n",
    "* Todos os valores de $h$ estão entre 0 e 1, então os logs serão negativos. Essa é a razão para o fator -1 aplicado à soma dos dois termos de perda.\n",
    "* Observe que quando o modelo prevê 1 ($h(z(\\theta)) = 1$) e o rótulo $y$ também é 1, a perda para esse exemplo de treinamento é 0.\n",
    "* Da mesma forma, quando o modelo prevê 0 ($h(z(\\theta)) = 0$) e o rótulo real também é 0, a perda para esse exemplo de treinamento é 0.\n",
    "* No entanto, quando a previsão do modelo está perto de 1 ($h(z(\\theta)) = 0.9999$) e o rótulo é 0, o segundo termo da perda de log torna-se um grande número negativo, que é então multiplicado pelo fator geral de -1 para convertê-lo em um valor de perda positivo. $-1 \\times (1 - 0) \\times log(1 - 0,9999)\\approx 9.2$ Quanto mais perto a previsão do modelo chega de 1, maior a perda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.210340371976294"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifique que quando o modelo prevê perto de 1, mas o rótulo real é 0, a perda é um grande valor positivo\n",
    "-1 * (1 - 0) * np.log(1 - 0.9999) # perda (loss) = aprox. 9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Da mesma forma, se o modelo prevê perto de 0 ($h(z) = 0.0001 $), mas o rótulo real é 1, o primeiro termo na função de perda torna-se um grande número: $-1 \\times log(0.0001) \\approx 9.2$. Quanto mais próxima de zero a previsão estiver, maior será a perda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.210340371976182"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifique se quando o modelo prevê perto de 0, mas o rótulo real é 1, a perda é um grande valor positivo\n",
    "-1 * np.log(0.0001) # perda (loss) é próxima de 9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Atualizando os pesos\n",
    "\n",
    "Para atualizar seu vetor de peso $ \\theta$, você aplicará gradiente descendente para melhorar iterativamente as previsões do seu modelo.\n",
    "O gradiente da função de custo $ J $ em relação a um dos pesos $ \\theta_j $ é:\n",
    "\n",
    "$$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)}-y^{(i)})x_j \\tag{5}$$\n",
    "* 'i' é o índice de todos os exemplos de treinamento 'm'.\n",
    "* 'j' é o índice do peso $ \\theta_j$, então $ x_j $ é a característica associada ao peso $ \\theta_j $\n",
    "\n",
    "* Para atualizar o peso $ \\theta_j $, nós o ajustamos subtraindo uma fração do gradiente determinado por $ \\alpha $:\n",
    "$$\\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j}J(\\theta) $$\n",
    "* A taxa de aprendizado $\\alpha $ é um valor que escolhemos para controlar o tamanho de uma única atualização."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementando a função de descida gradiente\n",
    "* O número de iterações `num_iters` é o número de vezes que você usará todo o conjunto de treinamento.\n",
    "* Para cada iteração, você calculará a função de custo usando todos os exemplos de treinamento (existem exemplos de treinamento `m`) e para todos os recursos.\n",
    "* Em vez de atualizar um único peso $ \\theta_i $ por vez, podemos atualizar todos os pesos no vetor coluna:\n",
    "$$\\mathbf{\\theta} = \\begin{pmatrix}\n",
    "\\theta_0\n",
    "\\\\\n",
    "\\theta_1\n",
    "\\\\ \n",
    "\\theta_2 \n",
    "\\\\ \n",
    "\\vdots\n",
    "\\\\ \n",
    "\\theta_n\n",
    "\\end{pmatrix}$$\n",
    "* $\\mathbf{\\theta}$ tem dimensões (n + 1, 1), onde 'n' é o número de recursos, e há mais um elemento para o termo de polarização $ \\theta_0 $ (observe que o valor do recurso correspondente $ \\mathbf {x_0} $ é 1 )\n",
    "* Os 'logits', 'z', são calculados multiplicando a matriz de características 'x' com o vetor de peso 'theta'. $ z =\\mathbf{x}\\mathbf{\\theta}$\n",
    "    * $\\mathbf{x}$ has dimensions (m, n+1) \n",
    "    * $\\mathbf{\\theta}$: has dimensions (n+1, 1)\n",
    "    * $\\mathbf{z}$: has dimensions (m, 1)\n",
    "* A previsão 'h' é calculada pela aplicação do sigmóide a cada elemento em 'z': $ h (z) = sigmóide (z) $, e tem dimensões (m, 1).\n",
    "* A função de custo $ J $ é calculada tomando o produto escalar dos vetores 'y' e 'log (h)'. Uma vez que 'y' e 'h' são vetores coluna (m, 1), transponha o vetor para a esquerda, de modo que a multiplicação da matriz de um vetor linha com vetor coluna execute o produto escalar.\n",
    "$$J = \\frac{-1}{m} \\times \\left(\\mathbf{y}^T \\cdot log(\\mathbf{h}) + \\mathbf{(1-y)}^T \\cdot log(\\mathbf{1-h}) \\right)$$\n",
    "* A atualização de theta também é vetorizada. Como as dimensões de $ \\mathbf {x} $ são (m, n + 1), e ambos $ \\mathbf {h} $ e $ \\mathbf {y} $ são (m, 1), precisamos transpor o $ \\mathbf{x}$ e coloque-o à esquerda para realizar a multiplicação da matriz, que então produz a resposta (n + 1, 1) que precisamos:\n",
    "$$\\mathbf{\\theta} = \\mathbf{\\theta} - \\frac{\\alpha}{m} \\times \\left( \\mathbf{x}^T \\cdot \\left( \\mathbf{h-y} \\right) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Vamos usar np.dot para multiplicação de matrizes.** \n",
    "- **Para garantir que a fração -1 / m seja um valor decimal, lance o numerador ou denominador (ou ambos), como `float (1)`, ou escreva `1.` para a versão flutuante de 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(x, y, theta, alpha, num_iters):\n",
    "    '''\n",
    "     Input:\n",
    "         x: matriz de recursos que é (m, n + 1)\n",
    "         y: rótulos correspondentes da matriz de entrada x, dimensões (m, 1)\n",
    "         theta: vetor peso da dimensão (n + 1,1)\n",
    "         alfa: taxa de aprendizagem\n",
    "         num_iters: número de iterações para as quais você deseja treinar seu modelo\n",
    "     Output:\n",
    "         J: o custo final\n",
    "         theta: seu vetor de peso final\n",
    "     Dica: você pode querer imprimir o custo para ter certeza de que está diminuindo.\n",
    "    '''\n",
    "\n",
    "    # # obtendo 'm', o número de linhas na matriz x\n",
    "    m = len(x)\n",
    "    #m = x.shape[0]\n",
    "    \n",
    "    for i in range(0, num_iters):\n",
    "        \n",
    "        # obtendo z, o produto escalar (dot product) de x e theta\n",
    "        z = np.dot(x, theta)\n",
    "        \n",
    "        # obtendo o sigmoide de z\n",
    "        h = sigmoid(z)\n",
    "        \n",
    "        # calculando a função custo (cost function)\n",
    "        J = (-1/float(m)) * (np.dot(np.transpose(y),np.log(h)) + np.dot(np.transpose(1-y),np.log(1-h)))\n",
    "\n",
    "        # atualizando o peso de theta\n",
    "        theta = theta - (alpha/float(m)) * (np.dot(np.transpose(x),(h-y)))\n",
    "\n",
    "    J = float(J)\n",
    "    return J, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost after training is 0.67094970.\n",
      "The resulting vector of weights is [4.1e-07, 0.00035658, 7.309e-05]\n"
     ]
    }
   ],
   "source": [
    "# Check the function\n",
    "# Construct a synthetic test case using numpy PRNG functions\n",
    "np.random.seed(1)\n",
    "# X input is 10 x 3 with ones for the bias terms\n",
    "tmp_X = np.append(np.ones((10, 1)), np.random.rand(10, 2) * 2000, axis=1)\n",
    "# Y Labels are 10 x 1\n",
    "tmp_Y = (np.random.rand(10, 1) > 0.35).astype(float)\n",
    "\n",
    "# Apply gradient descent\n",
    "tmp_J, tmp_theta = gradientDescent(tmp_X, tmp_Y, np.zeros((3, 1)), 1e-8, 700)\n",
    "print(f\"The cost after training is {tmp_J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(tmp_theta)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: Extraindo os recursos\n",
    "\n",
    "* Dada uma lista de tweets, vamos extrair os recursos e armazenar em uma matriz. Vamos extrair dois recursos.\n",
    "     * O primeiro recurso é o número de palavras positivas em um tweet.\n",
    "     * O segundo recurso é o número de palavras negativas em um tweet.\n",
    "* Em seguida, vamos treinar o classificador de regressão logística nesses recursos.\n",
    "* E testar o classificador em um conjunto de validação.\n",
    "\n",
    "### Implementando a função extract_features.\n",
    "* Esta função aceita um único tweet.\n",
    "* Processaremos o tweet usando a função importada `process_tweet ()` e salvaremos a lista de palavras do tweet.\n",
    "* Vamos realizar um loop em cada palavra na lista de palavras processadas\n",
    "     * Para cada palavra, vamos verificar o dicionário `freqs` para saber a contagem quando essa palavra tiver um rótulo '1' positivo. (Verifique a chave (word, 1.0)\n",
    "     * Vamos fazer o mesmo para a contagem de quando a palavra estiver associada ao rótulo negativo '0'. (Verifique a chave (word, 0,0).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos também lidar com os casos em que a chave (word, label) não é encontrada no dicionário. Para isso utilizamos o método `.get()`, lembra?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet, freqs):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: uma lista de palavras para um tweet\n",
    "        freqs: um dicionário correspondente às frequências de cada tupla (word, label)\n",
    "    Output: \n",
    "        x: um vetor de features de dimensão (1,3)\n",
    "    '''\n",
    "    # process_tweet tokeniza, faz stemização e remove stopwords\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    # 3 elementos (0, 0, 0) na forma de um vetor 1 x 3\n",
    "    x = np.zeros((1, 3)) \n",
    "    \n",
    "    #bias é 1 (primeiro elemento)\n",
    "    x[0,0] = 1 \n",
    " \n",
    "    # loop por cada palavra na lista de palavras\n",
    "    for word in word_l:\n",
    "\n",
    "        # para a contagem de palavras, incrementa se positivo\n",
    "        x[0,1] += freqs.get((word, 1.0), 0)\n",
    "        \n",
    "        # para a contagem de palavras, incrementa se negativo\n",
    "        x[0,2] += freqs.get((word, 0.0), 0)\n",
    "        \n",
    "\n",
    "    assert(x.shape == (1, 3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00e+00 3.02e+03 6.10e+01]]\n"
     ]
    }
   ],
   "source": [
    "# teste 1\n",
    "# testando no training data\n",
    "tmp1 = extract_features(train_x[0], freqs)\n",
    "print(tmp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# teste 2:\n",
    "# verificando se as palavras não estão no dicionário de freqs\n",
    "tmp2 = extract_features('blorb bleeeeb bloooob', freqs)\n",
    "print(tmp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Treinando o Modelo\n",
    "Para treinar o modelo:\n",
    "* Vamos empilhar os recursos para todos os exemplos de treinamento em uma matriz `X`.\n",
    "* E chamar `gradientDescent`, que implementamos acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost after training is 0.24216477.\n",
      "The resulting vector of weights is [7e-08, 0.0005239, -0.00055517]\n"
     ]
    }
   ],
   "source": [
    "# coletando os recursos 'x' e empilhando em uma matriz 'X'\n",
    "X = np.zeros((len(train_x), 3))\n",
    "for i in range(len(train_x)):\n",
    "    X[i, :]= extract_features(train_x[i], freqs)\n",
    "\n",
    "# as etiquetas de treinamento correspondentes a X\n",
    "Y = train_y\n",
    "\n",
    "# aplicando gradientDescent\n",
    "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
    "print(f\"The cost after training is {J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado esperado**: \n",
    "\n",
    "```\n",
    "The cost after training is 0.24216529.\n",
    "The resulting vector of weights is [7e-08, 0.0005239, -0.00055517]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testando a regressão logística\n",
    "\n",
    "Vamos **testar** a função de regressão logística em alguma nova entrada que nosso modelo não viu antes e **prever** se um tweet é positivo ou negativo.\n",
    "\n",
    "* Para cada tweet, processar e extrair os recursos.\n",
    "* Aplicar os pesos aprendidos do modelo nos recursos para obter os logits.\n",
    "* Aplicar o sigmóide aos logits para obter a previsão (um valor entre 0 e 1).\n",
    "\n",
    "$$y_{pred} = sigmoid(\\mathbf{x} \\cdot \\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, freqs, theta):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: uma string\n",
    "        freqs: dicionário de frequências (word, label)\n",
    "        theta: vetor de pesos (3,1) \n",
    "    Output: \n",
    "        y_pred: probabilidade de um tweet ser positivo ou negativo\n",
    "    '''\n",
    "\n",
    "    # extraindo features do tweet e colocando em \"x\"\n",
    "    x = extract_features(tweet, freqs)\n",
    "    \n",
    "    # realizando a predição com x e theta utilizando .dot\n",
    "    y_pred = sigmoid(np.dot(x, theta))\n",
    "      \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am happy -> 0.518580\n",
      "I am bad -> 0.494339\n",
      "this movie should have been great. -> 0.515331\n",
      "great -> 0.515464\n",
      "great great -> 0.530898\n",
      "great great great -> 0.546273\n",
      "great great great great -> 0.561561\n"
     ]
    }
   ],
   "source": [
    "# Vamos testar nossa função\n",
    "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
    "    print( '%s -> %f' % (tweet, predict_tweet(tweet, freqs, theta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.51349316]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aqui você pode testar suas próprias frases. A função predict_tweet está funcionando!\n",
    "my_tweet = 'NLP is so good'\n",
    "predict_tweet(my_tweet, freqs, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando o desempenho usando o _test set_\n",
    "Depois de treinar o modelo usando o conjunto de treinamento acima, vamos verificar como o modelo pode funcionar em dados reais e ainda não vistos, testando-o em relação ao conjunto de teste.\n",
    "\n",
    "* Dados os dados de teste e os pesos do modelo treinado, vamos calcular a precisão do modelo de regressão logística.\n",
    "* Vamos usar a função `predict_tweet ()` para fazer previsões em cada tweet no conjunto de teste.\n",
    "* Se a previsão for > 0,5, a classificação do modelo `y_hat` será 1, caso contrário, será 0.\n",
    "* Uma previsão é ACURADA quando `y_hat` é igual a` test_y`. Somaremos todas as instâncias quando forem iguais e divida por `m`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos np.asarray() para converter a lista em um array numpy\n",
    "Usaremos np.squeeze() para transformar o array dimensional (m,1) em (m,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        test_x: lista de tweets\n",
    "        test_y: vetor (m, 1) com as etiquetas de x\n",
    "        freqs: dicionário de frequências pos e neg\n",
    "        theta: vetor de pesos de dimensão (3, 1)\n",
    "    Output: \n",
    "        accuracy: (tweets classificados corretamente) / (total de tweets)\n",
    "    \"\"\"\n",
    "    \n",
    "    # lista para colocar as predições\n",
    "    y_hat = []\n",
    "    \n",
    "    for tweet in test_x:\n",
    "        # conseguindo a predição de qual etiqueta será\n",
    "        y_pred = predict_tweet(tweet, freqs, theta)\n",
    "        \n",
    "        if y_pred > 0.5:\n",
    "            # append 1.0 para a lista\n",
    "            y_hat.append(1.0)\n",
    "        else:\n",
    "            # append 0 para a lista\n",
    "            y_hat.append(0)\n",
    "\n",
    "     # Com a implementação acima, y_hat é uma lista, mas test_y é array (m, 1), então...\n",
    "     # vamos converter ambos em arrays unidimensionais para compará-los usando o operador '=='\n",
    "    accuracy = sum((np.asarray(y_hat) == np.squeeze(test_y))) / len(test_y)\n",
    "   \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression model's accuracy = 0.9950\n"
     ]
    }
   ],
   "source": [
    "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
    "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 5: Análise de Erro\n",
    "\n",
    "Nesta parte, veremos quais os tweets que nosso modelo classificou errado. \n",
    "\n",
    "Por que você acha que as classificações erradas aconteceram? Especificamente, que tipo de tweets o modelo classificou incorretamente?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Predicted Tweet\n",
      "THE TWEET IS: @jaredNOTsubway @iluvmariah @Bravotv Then that truly is a LATERAL move! Now, we all know the Queen Bee is UPWARD BOUND : ) #MovingOnUp\n",
      "THE PROCESSED TWEET IS: ['truli', 'later', 'move', 'know', 'queen', 'bee', 'upward', 'bound', 'movingonup']\n",
      "1\t0.49996897\tb'truli later move know queen bee upward bound movingonup'\n",
      "THE TWEET IS: @MarkBreech Not sure it would be good thing 4 my bottom daring 2 say 2 Miss B but Im gonna be so stubborn on mouth soaping ! #NotHavingit :p\n",
      "THE PROCESSED TWEET IS: ['sure', 'would', 'good', 'thing', '4', 'bottom', 'dare', '2', 'say', '2', 'miss', 'b', 'im', 'gonna', 'stubborn', 'mouth', 'soap', 'nothavingit', ':p']\n",
      "1\t0.48650628\tb'sure would good thing 4 bottom dare 2 say 2 miss b im gonna stubborn mouth soap nothavingit :p'\n",
      "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots\n",
      "http://t.co/UGQzOx0huu\n",
      "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
      "1\t0.48370676\tb\"i'm play brain dot braindot\"\n",
      "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots http://t.co/aOKldo3GMj http://t.co/xWCM9qyRG5\n",
      "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
      "1\t0.48370676\tb\"i'm play brain dot braindot\"\n",
      "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots http://t.co/R2JBO8iNww http://t.co/ow5BBwdEMY\n",
      "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
      "1\t0.48370676\tb\"i'm play brain dot braindot\"\n",
      "THE TWEET IS: off to the park to get some sunlight : )\n",
      "THE PROCESSED TWEET IS: ['park', 'get', 'sunlight']\n",
      "1\t0.49578773\tb'park get sunlight'\n",
      "THE TWEET IS: @msarosh Uff Itna Miss karhy thy ap :p\n",
      "THE PROCESSED TWEET IS: ['uff', 'itna', 'miss', 'karhi', 'thi', 'ap', ':p']\n",
      "1\t0.48199817\tb'uff itna miss karhi thi ap :p'\n",
      "THE TWEET IS: @phenomyoutube u probs had more fun with david than me : (\n",
      "THE PROCESSED TWEET IS: ['u', 'prob', 'fun', 'david']\n",
      "0\t0.50020361\tb'u prob fun david'\n",
      "THE TWEET IS: pats jay : (\n",
      "THE PROCESSED TWEET IS: ['pat', 'jay']\n",
      "0\t0.50039294\tb'pat jay'\n",
      "THE TWEET IS: my beloved grandmother : ( https://t.co/wt4oXq5xCf\n",
      "THE PROCESSED TWEET IS: ['belov', 'grandmoth']\n",
      "0\t0.50000002\tb'belov grandmoth'\n"
     ]
    }
   ],
   "source": [
    "print('Label Predicted Tweet')\n",
    "for x,y in zip(test_x,test_y):\n",
    "    y_hat = predict_tweet(x, freqs, theta)\n",
    "\n",
    "    if np.abs(y - (y_hat > 0.5)) > 0:\n",
    "        print('THE TWEET IS:', x)\n",
    "        print('THE PROCESSED TWEET IS:', process_tweet(x))\n",
    "        print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 6: Preveja com seu próprio tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['impress', 'passag', 'time', 'mathemat', 'longer', 'nuisanc', 'great', 'relief']\n",
      "[[0.51022075]]\n",
      "Positive sentiment\n"
     ]
    }
   ],
   "source": [
    "# Sinta-se à vontade para alterar o tweet abaixo\n",
    "my_tweet = 'I have the impression that with the passage of time, mathematics will no longer be a nuisance, to be a great relief!'\n",
    "print(process_tweet(my_tweet))\n",
    "y_hat = predict_tweet(my_tweet, freqs, theta)\n",
    "print(y_hat)\n",
    "if y_hat > 0.5:\n",
    "    print('Positive sentiment')\n",
    "else: \n",
    "    print('Negative sentiment')"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "NLPC1-1"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
