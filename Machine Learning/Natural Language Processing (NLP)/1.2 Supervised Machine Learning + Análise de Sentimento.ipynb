{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Supervised Machine Learning + Análise de Sentimento\n",
    "\n",
    "\n",
    "Quando tratamos de Supervised Machine Learning (aprendizado de máquina supervisionado), não há como fugir da **Regressão Logística**, uma técnica estatística de predição de valores. **Em NLP não é diferente**. Vamos compreender as etapas necessárias para implementar esse algoritmo.\n",
    "\n",
    "Para realizar um Supervised ML, temos dois elementos principais:\n",
    "1. Features (X) (o input);\n",
    "2. Labels (Y) (o output).\n",
    "\n",
    "Para que tenhamos predições acuradas baseadas nos dados disponíveis, precisamos minimizar nossas taxas de erro (error rates) ou custo.\n",
    "\n",
    "Através das Features (X), executamos nossa função de predição (que leva em conta os dados de parâmetros) para realizar o output nas Labels (Y). **A tarefa é fazer com que o valor esperado de Y e o valor predito em Y tenha o mínimo de distância possível**. \n",
    "\n",
    "A **função de custo/perda** (**cost/loss function**) compara o quão próximas estão os Outputs e as Labels Y. Os parâmetros são atualizados até que o custo seja minimizado.\n",
    "\n",
    "<img src=\"images/SML.png\" width='50%'>\n",
    "\n",
    "### 1.2.1 Análise de sentimento\n",
    "Análise de sentimento ou sentiment analysis pode ser obtido através do Supervised Machine Learning (SVM). Vamos **analisar o sentimento de tweets**. Um exemplo:\n",
    "\n",
    "Temos dois tweets:\n",
    "1. \"Eu gosto muito de estudar\";\n",
    "2. \"Eu gosto de estudar, mas quando me deparo com matemática fico muito triste\".\n",
    "\n",
    "Qual você acha que deveria ser o Output de nossa análise de sentimento automática para cada uma dessas sentenças? **Em um Supervised Machine Learning, é você que decidirá o Output.** Ou seja, é quem está treinando o modelo que definirá qual as Labels (Y), para que o algoritmo então consiga imitar.\n",
    "\n",
    "Vamos dizer então que o primeiro tweet é **positivo** e o segundo, apesar de um pouco mais complexo, **negativo**. Positivo será \"1\" e negativo \"0\", ou seja, valores binários.\n",
    "\n",
    "Vamos então utilizar o modelo de classificação da **regressão logística** para predizer o sentimento de um tweet:\n",
    "1. primeiro precisamos colher e processar os tweets (que estão em texto cru) para extrair features;\n",
    "2. então treinamos o modelo de regressão logística, e isso inclui repetir até minimizar o custo (cost);\n",
    "3. realizar as predições com tweets ainda não utilizados!\n",
    "\n",
    "<img src=\"images/SML train.png\" width='50%'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Representando um texto como vetor\n",
    "Para representar um texto como vetor (e assim encodar qualquer texto como um array de números), primeiramente precisamos construir um **vocabulário**. Dentro do vocabulário estarão todas as palavras (unique words) de sua coleção de tweets.\n",
    "\n",
    "Vamos representar o array do conjunto dos dois tweets apresentados anteriormente.\n",
    "\n",
    "**V** = [Eu, gosto, muito, de, estudar, mas, quando, me, deparo, com, matemática, fico, triste]\n",
    "\n",
    "Repare que todas as palavras utilizadas estão dentro do array, porém não se repetem. Através do vetor, podemos realizar a **extração de features** manualmente para entender melhor como funciona!\n",
    "\n",
    "1. \"Eu gosto muito de estudar\"\n",
    "\n",
    "|Eu| gosto| muito| de| estudar| mas| quando| me| deparo| com| matemática| fico| triste|\n",
    "|-|-|-|-|-|-|-|-|-|-|-|-|-|\n",
    "|1|1|1|1|1|0|0|0|0|0|0|0|0|\n",
    "\n",
    "2. \"Eu gosto de estudar, mas quando me deparo com matemática fico muito triste\".\n",
    "\n",
    "|Eu| gosto| muito| de| estudar| mas| quando| me| deparo| com| matemática| fico| triste|\n",
    "|-|-|-|-|-|-|-|-|-|-|-|-|-|\n",
    "|1|1|1|1|1|1|1|1|1|1|1|1|1|\n",
    "\n",
    "A segunda sentença possui \"1\" em todas as palavras de nosso vetor, pois contém todas as palavras em sua sentença. \n",
    "\n",
    "Essa forma de representação se chama **sparse representation**.\n",
    "\n",
    "Podemos imaginar que, quanto mais tweets implementarmos em nosso _dataset_, **mais zeros** teremos. Em um vocabulário de 500 palavras diferentes (portanto 500 parâmetros), uma sentença como a primeira teria 495 \"zeros\", e cinco \"uns\". Esse modo de representação faz com que o tempo de treinamento **seja muito longo**.\n",
    "\n",
    "<img src=\"images/sparse representation.png\" width='50%'>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Um novo modo de encodar os textos\n",
    "Para nosso classificador de regressão logística, podemos realizar a **contagem de positivos e negativos**. Vamos a um novo exemplo.\n",
    "1. Eu gosto de NLP, estou feliz;\n",
    "2. Eu estou feliz;\n",
    "3. Estou triste, NLP é exigente;\n",
    "4. Estou triste.\n",
    "\n",
    "- Separamos os tweets positivos dos negativos. Vamos considerar o vocabulário e contar quantas vezes se repetem as palavras em cada uma das duas classificações:\n",
    "\n",
    "| Vocabulário | PosFreq (1) | NegFreq (0) |\n",
    "|-|-|-|\n",
    "|Eu|2 |0|\n",
    "| gosto|1 |0|\n",
    "| de |1 |0|\n",
    "|NLP |1 |1|\n",
    "| Estou |2 |2|\n",
    "|feliz |2 |0|\n",
    "| triste | 0 |2|\n",
    "| é | 0 |1|\n",
    "| exigente | 0 |1|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Extraindo features utilizando frequências\n",
    "Agora teremos um incremento na velocidade de classificação, pois ao invés de aprender **V** features (de todo o vocabulário, classificados em \"zero\" e \"um\" por palavra), nosso modelo precisará extrair apenas **TRÊS** features por tweet:\n",
    "1. bias unit (unidade de polarização) igual a \"1\";\n",
    "2. soma de todas PosFrequencies para cada unique word (como calculado acima);\n",
    "2. soma de todas NegFrequencies para cada unique word.\n",
    "\n",
    "Estamos começando a entender o que acontece e como o modelo escolhe uma classificação. Se pegarmos as PosFreq (1), podemos observar que as palavras feliz (2 vezes) e gosto (1 vez) não aparecem nas frases negativas.\n",
    "\n",
    "Vamos extrair manualmente as features de nosso tweet \"Estou triste, NLP é exigente\".\n",
    "1. da coluna de PosFreq(1) somamos as frequências que aparecem na sentença negativa:\n",
    "    1. \"Estou\" = 2, \"triste\" = 0, \"NLP\" = 1, \"é\" = 0, \"exigente\" = 0. **Soma = 3**.\n",
    "2. vamos fazer o mesmo para as NegFreq(0):\n",
    "    1. \"Estou\" = 2, \"triste\" = 2, \"NLP\" = 1, \"é\" = 1, \"exigente\" = 1. **Soma = 7**.\n",
    "    \n",
    "| PosFreq(1) | NegFreq(0) |\n",
    "|:-:|:-:|\n",
    "|<img src=\"images/posfreq.png\" width='85%'>|<img src=\"images/negfreq.png\" width='85%'>|\n",
    "\n",
    "    \n",
    "Sendo assim, a representação da Feature extraction do tweet negativo acima será o Vetor = [1, 3, 7]\n",
    "\n",
    "**Ao invés de 500 features, agora temos apenas três!**\n",
    "\n",
    "No próximo guia, vamos definir e ver alguns exemplos de **pré-processamento de texto**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
