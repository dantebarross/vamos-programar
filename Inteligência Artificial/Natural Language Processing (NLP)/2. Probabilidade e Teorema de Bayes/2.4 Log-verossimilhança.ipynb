{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Log-verossimilhança\n",
    "\n",
    "\n",
    "Agora que já **compreendemos** o que é e calcular o **prior ratio**, temos então a **fórmula completa** do Naive Bayes para classificações binárias! Vamos apenas fazer mais algumas considerações:\n",
    "* Calcular a probabilidade de sentimentos requer a multiplicação de vários números entre \"0\" e \"1\", e fazer isso no computador pode causar o chamado **underflow numérico**, quando um número é tão pequeno que não pode ser armazenado na máquina. Para isso, podemos utilizar um **truque matemático** para resolver isso. Esse truque se chama **log-verossimilhança**.\n",
    "\n",
    "As **log-verossimilhanças** (**log likelihoods**) são os logaritmos das probabilidades. Nesse caso, das probabilidades do guia passado. Elas costumam aparecer ao longo dos projetos de deep-learning e também especificamente em NLP.\n",
    "\n",
    "A fórmula que estamos utilizando para calcular a pontuação para o **Naive Bayes** é a **prior ratio** multiplicada pela **verossimilhança**:\n",
    "\n",
    "<img src=\"images/naive bayes formula.png\" width=25%>\n",
    "\n",
    "Para realizar o \"truque\", ao invés de utilizar a **pontuação crua**, utilizamos o **log** da pontuação. Por fim, basta somarmos o log da **prior** com o log da **likelihood** (verossimilhança). Ou seja, é a soma dos logaritmos da razão das probabilidades condicionais, de todas as palavras únicas no corpus.\n",
    "\n",
    "<img src=\"images/naive bayes formula2.png\" width=60%>\n",
    "\n",
    "### 2.4.1 Classificando os tweets com lambda\n",
    "Nós já utilizamos o \"Naive Bayes inference condition\" em guias passados para conquistarmos a pontuação sentimental de nossos tweets. **Vamos fazer algo muito similar para conseguir o log dessa pontuação**.\n",
    "\n",
    "Para isso, vamos calcular algo chamado **lambda**, que é o log da razão das probabilidades positivas dividido pelas probabilidades negativas:\n",
    "\n",
    "$\\lambda(w) = log \\dfrac{P(w|pos)}{P(w|neg)}$\n",
    "\n",
    "Por fim, chegamos aos seguintes **lambdas**:\n",
    "<img src=\"images/naive bayes tabela probabilidade5.png\" width=30%>\n",
    "\n",
    "\n",
    "### 2.4.1 Relembrando\n",
    "Vamos relembrar o que fizemos até agora:\n",
    "* Palavras podem ser ambíguas, portanto dividindo-as em três categorias podemos ter uma confiança maior quando formos classificá-las;\n",
    "* Desse modo, caso uma palavra apareça em quantidades iguais tanto em Pos quanto em Neg, ela é uma palavra Neutra;\n",
    "* Para chegar a esses resultados, nós calculamos as **probabilidades condicionais** das palavras em cada categoria (Pos e Neg);\n",
    "* Calculamos então a **razão** entre essas probabilidades condicionais.\n",
    "* Expressamos essa **razão** como um logaritmo chamado **lambda** para reduzir o risco do fenômeno **underflow numérico**.\n",
    "\n",
    "### 2.4.2 Calculando a log-verossimilhança\n",
    "A **log-verossimilhança** (log-likelihood) pode ser calculada basicamente **somando os lambdas de todas as palavras de um tweet**. Tendo como base a tabela acima, podemos chegar ao log-verossimilhança da frase \"I am happy because I am learning\" através do cálculo abaixo:\n",
    "* 0 + 0 + 2.2 + 0 + 0 + 0 + 1.1 = **3.3**;\n",
    "* Esse valor é maior do que zero, portanto...\n",
    "* O tweet é classificado como **positivo**.\n",
    "\n",
    "Anteriormente, estávamos considerando o \"1\" como neutro. Agora utilizando a **log-verossimilhança**:\n",
    "* o \"0\" é o número neutro;\n",
    "* os negativos são os números negativos;\n",
    "* positivos são números positivos.\n",
    "\n",
    "Percebeu como as palavras **happy** e **learning** influenciaram na pontuação final? As outras são todas palavras neutras, sem influência alguma!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
